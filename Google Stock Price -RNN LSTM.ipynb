{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM model that will try to predict downward and upward trends of the Google stock price. We are going to train our model on the  5 year Google stock price (from beginning of 2012 to the end of 2016). We will try to predict the January 2017 stock price upward/downward trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data preprocessing  \n",
    "### 2) Building the RNN\n",
    "### 3) Making the predictions and visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1: IMPORTING THE LIBRARIES\n",
    "import numpy as np #we will make arrays. they are the only allowed inputs\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2: IMPORT THE TRAINING SET:\n",
    "dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')\n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "# dataset_train: we only need the second column (open = [1]) to train our dataset.\n",
    "#.iloc[:, 1:2].values: means all rows and and first column\n",
    "#1:2: if we just use 1 here, instead of 1:2, we create an array with 1 row. so all the column information is becomes one row. We don't want that\n",
    "#.values provides us to turn this data frame into a numpy array. without it the output is still a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3: FEATURE SCALING:\n",
    "# we use normalisation scaling technique, instead of standardisation. This is recommended in this type of analysis\n",
    "# whenever you build an RNN, especially when the sigmoid function is the activation function, use normalisation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "#feature range = (0,1) is the default. because all the new/normalized stock prices will be between 0 and 1\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4: CREATING A DATA STRUCTURE WITH 60 TIMESTEPS AND 1 OUTPUT\n",
    "# we are going to create a data structure specifying what the RNN will need to remember when predicting the next stock price\n",
    "#and this is called the number of timesteps. this is super important to have the correct number of timestamps cause it could lead to overfitting\n",
    "# 60 timesteps mean that at each time t the RNN is going to look at 60 stock prices before time t\n",
    "# the stock price is between 60 days before time t and time t. \n",
    "#based on the trends it's capturing during this 60 days timesteps, it will try to predict the next output\n",
    "# 60 is the experimented timestep for this prediction\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# X_train is the input \n",
    "# y_train is the output\n",
    "# range(60, 1258): between 60th financial day and 1258 is the last index in our set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5: RESHAPING:\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "#input shape: the input should be a 3d array\n",
    "#adding some more dimensionality. we will add the unit. \n",
    "#that is the number of predictors we want\n",
    "#X_train.shape[0]: batch size= the number of observations. 1198 observations. you can have this in any dataset. because this line gives the number of rows\n",
    "#X_train.shape[1]: number of timesteps: this is the number of columns\n",
    "#1: number of indicators= number of predictors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building the RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1: IMPORTING KERAS LIBRARIES AND PACKAGES:\n",
    "from keras.models import Sequential # allow us to create a neural network object representing a sequence of layers\n",
    "from keras.layers import Dense #to add the output layer \n",
    "from keras.layers import LSTM # to add LSTM layer\n",
    "from keras.layers import Dropout #to add some dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2: INITIALISING THE RNN:\n",
    "regressor = Sequential()\n",
    "#since we are predicting a continous value, we are doing some regression. therefore we use the name regressor for our model. \n",
    "#classification, on the other hand, is about predicting a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3: ADDING THE FIRST LSTM LAYER AND SOME DROPOUT REGULARIZATION:\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))  \n",
    "regressor.add(Dropout(0.2))\n",
    "#arguments:\n",
    "#units: the number of LSTM cells, or memory units (neurons) you want to have in this LSTM layer. \n",
    "# 50 neurons in this layer will get us a model with high dimentiality\n",
    "# return_sequences: it's true because we are building a stacked LSTM which therefore have several LSTM layers\n",
    "#return sequences is true because we are adding another LSTM layer = making a stacked LSTM\n",
    "# once you are done with the LSTM layers, when you are not going to add another one, you will set it to false. false is the default value \n",
    "# input_shape: the shape of the input that we created in the last part\n",
    "#it's an input shape of three dimensions, corresponding to observations(number of rows), the timestep (number of columns) and the indicators.\n",
    "#we don't have to include all the three dimensions \"(X_train.shape[0], X_train.shape[1], 1)\" we will just add the last two. \n",
    "#because the first one is corresponding to the observations, will be automatically taken into account\n",
    "# Dropout(0.2): the rate of neurons, you want to ignore in the layers. 0.2 is used generally. 20% dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4: ADDING THE SECOND, THIRD AND FOURTH LSTM LAYERS AND DROPOUTS:\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularization\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# we only specify the shape of the input in the first layer\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularization\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularization\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5: ADDING THE OUTPUT LAYER:\n",
    "regressor.add(Dense(units = 1))\n",
    "# to make a full connection with the last lstm layer and the output, we use the dense class\n",
    "#unit =1 the number of neurons that needs to be in the output\n",
    "#since we're predicting a real value corresponding to the stock price, the output has only one dimension (neuron).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.6: COMPILING THE RNN WITH THE OPTIMIZER AND THE LOSS:\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "#compile is a method of sequential class\n",
    "#mean_squared_error is good for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 23s 166ms/step - loss: 0.0412\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 6s 150ms/step - loss: 0.0062\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 6s 153ms/step - loss: 0.0054\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 6s 156ms/step - loss: 0.0050\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 6s 161ms/step - loss: 0.0052\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 0.0052\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 0.0052\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 0.0049\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 7s 171ms/step - loss: 0.0040\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 7s 171ms/step - loss: 0.0040\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 6s 160ms/step - loss: 0.0040\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 7s 173ms/step - loss: 0.0040\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 8s 198ms/step - loss: 0.0038\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 7s 178ms/step - loss: 0.0040\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 0.0038\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0042\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.0038: 0s - l\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.0037\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.0037\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0036\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0036\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.0033\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.0033\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0034\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.0032\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0035\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0033\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0033\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0030\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0029\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0028\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0027\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0032\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0028\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0031\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0029\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0028\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0028\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0026\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0028\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0029\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0026\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 2s 57ms/step - loss: 0.0027\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 2s 57ms/step - loss: 0.0025\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0026\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 2s 60ms/step - loss: 0.0026\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 2s 62ms/step - loss: 0.0023\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 2s 56ms/step - loss: 0.0024\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0024\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 2s 58ms/step - loss: 0.0023\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 3s 66ms/step - loss: 0.0023\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0025\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0022\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0022\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.0020\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0021\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.0022\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.0020\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0019\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0020\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0020\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0018\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0018\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0020\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0021\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0021\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.0019\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.0023\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0019\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0020\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.0018\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0018\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0016\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0019\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0018\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0015\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0016\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0015\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.0018\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.0018\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0016\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.0016\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0016\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0016\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0018\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0016\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0015\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 0.0017\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0016\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0016\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0015\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0016\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0014\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.0016\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0014\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0015\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.0014\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0015\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 2s 56ms/step - loss: 0.0013\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2f3cbb82e20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.7 FITTING THE RNN TO THE TRAINING SET:\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n",
    "#we will update the stock prices in every 32 stock prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Making the predictions and visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1: Getting the real stock price of January 2017\n",
    "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "#we already know the stock prices for january 2017, but the model is not trained on this.\n",
    "\n",
    "# 3.2: Getting the predicted stock price of 2017\n",
    "#this is the predicted results for january 2017\n",
    "#1. key point:\n",
    "#we trained our model to be able to predict the stock prices according to previous 60 days\n",
    "#and therefore, to predict each stock price of each financial day of Jan 2017, we will need the 60 previous stock prices, before the actual day.\n",
    "\n",
    "#2. key point: \n",
    "#in order to get each day of jan 2017 the 60 previous stock prices, we will need both the training and test data\n",
    "#because we will have some of the 60 days from the training set, becuase they will be from Dec 2016 and we will also have some from the jan\n",
    "#therefore, we need concatinate the training and the test set.\n",
    "\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "# concat function: we united the two sets by rows (axis =0). we only have the 'open' column\n",
    "# inputs: the stock prices of the 60 previous stock prices (equal to 3 months of financial day)\n",
    "# len(dataset_total) - len(dataset_test) - 60: this is the lower bound\n",
    "# upper bound is the last index\n",
    "# inputs.reshape: this will get the inputs in one column\n",
    "# sc.transform(inputs): scaling the inputs\n",
    "\n",
    "X_test = []\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "\n",
    "#our test set only has 20 financial days (equals to 1 month=jan), therefore the upper bound is 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>325.25</td>\n",
       "      <td>332.83</td>\n",
       "      <td>324.97</td>\n",
       "      <td>663.59</td>\n",
       "      <td>7,380,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>331.27</td>\n",
       "      <td>333.87</td>\n",
       "      <td>329.08</td>\n",
       "      <td>666.45</td>\n",
       "      <td>5,749,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>329.83</td>\n",
       "      <td>330.75</td>\n",
       "      <td>326.89</td>\n",
       "      <td>657.21</td>\n",
       "      <td>6,590,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>328.34</td>\n",
       "      <td>328.77</td>\n",
       "      <td>323.68</td>\n",
       "      <td>648.24</td>\n",
       "      <td>5,405,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2012</td>\n",
       "      <td>322.04</td>\n",
       "      <td>322.29</td>\n",
       "      <td>309.46</td>\n",
       "      <td>620.76</td>\n",
       "      <td>11,688,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>12/23/2016</td>\n",
       "      <td>790.90</td>\n",
       "      <td>792.74</td>\n",
       "      <td>787.28</td>\n",
       "      <td>789.91</td>\n",
       "      <td>623,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>12/27/2016</td>\n",
       "      <td>790.68</td>\n",
       "      <td>797.86</td>\n",
       "      <td>787.66</td>\n",
       "      <td>791.55</td>\n",
       "      <td>789,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>12/28/2016</td>\n",
       "      <td>793.70</td>\n",
       "      <td>794.23</td>\n",
       "      <td>783.20</td>\n",
       "      <td>785.05</td>\n",
       "      <td>1,153,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>12/29/2016</td>\n",
       "      <td>783.33</td>\n",
       "      <td>785.93</td>\n",
       "      <td>778.92</td>\n",
       "      <td>782.79</td>\n",
       "      <td>744,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>12/30/2016</td>\n",
       "      <td>782.75</td>\n",
       "      <td>782.78</td>\n",
       "      <td>770.41</td>\n",
       "      <td>771.82</td>\n",
       "      <td>1,770,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Open    High     Low   Close      Volume\n",
       "0       1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
       "1       1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
       "2       1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
       "3       1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
       "4       1/9/2012  322.04  322.29  309.46  620.76  11,688,800\n",
       "...          ...     ...     ...     ...     ...         ...\n",
       "1253  12/23/2016  790.90  792.74  787.28  789.91     623,400\n",
       "1254  12/27/2016  790.68  797.86  787.66  791.55     789,100\n",
       "1255  12/28/2016  793.70  794.23  783.20  785.05   1,153,800\n",
       "1256  12/29/2016  783.33  785.93  778.92  782.79     744,300\n",
       "1257  12/30/2016  782.75  782.78  770.41  771.82   1,770,000\n",
       "\n",
       "[1258 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2017</td>\n",
       "      <td>778.81</td>\n",
       "      <td>789.63</td>\n",
       "      <td>775.80</td>\n",
       "      <td>786.14</td>\n",
       "      <td>1,657,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2017</td>\n",
       "      <td>788.36</td>\n",
       "      <td>791.34</td>\n",
       "      <td>783.16</td>\n",
       "      <td>786.90</td>\n",
       "      <td>1,073,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2017</td>\n",
       "      <td>786.08</td>\n",
       "      <td>794.48</td>\n",
       "      <td>785.02</td>\n",
       "      <td>794.02</td>\n",
       "      <td>1,335,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2017</td>\n",
       "      <td>795.26</td>\n",
       "      <td>807.90</td>\n",
       "      <td>792.20</td>\n",
       "      <td>806.15</td>\n",
       "      <td>1,640,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2017</td>\n",
       "      <td>806.40</td>\n",
       "      <td>809.97</td>\n",
       "      <td>802.83</td>\n",
       "      <td>806.65</td>\n",
       "      <td>1,272,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1/10/2017</td>\n",
       "      <td>807.86</td>\n",
       "      <td>809.13</td>\n",
       "      <td>803.51</td>\n",
       "      <td>804.79</td>\n",
       "      <td>1,176,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1/11/2017</td>\n",
       "      <td>805.00</td>\n",
       "      <td>808.15</td>\n",
       "      <td>801.37</td>\n",
       "      <td>807.91</td>\n",
       "      <td>1,065,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>807.14</td>\n",
       "      <td>807.39</td>\n",
       "      <td>799.17</td>\n",
       "      <td>806.36</td>\n",
       "      <td>1,353,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1/13/2017</td>\n",
       "      <td>807.48</td>\n",
       "      <td>811.22</td>\n",
       "      <td>806.69</td>\n",
       "      <td>807.88</td>\n",
       "      <td>1,099,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1/17/2017</td>\n",
       "      <td>807.08</td>\n",
       "      <td>807.14</td>\n",
       "      <td>800.37</td>\n",
       "      <td>804.61</td>\n",
       "      <td>1,362,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1/18/2017</td>\n",
       "      <td>805.81</td>\n",
       "      <td>806.21</td>\n",
       "      <td>800.99</td>\n",
       "      <td>806.07</td>\n",
       "      <td>1,294,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1/19/2017</td>\n",
       "      <td>805.12</td>\n",
       "      <td>809.48</td>\n",
       "      <td>801.80</td>\n",
       "      <td>802.17</td>\n",
       "      <td>919,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1/20/2017</td>\n",
       "      <td>806.91</td>\n",
       "      <td>806.91</td>\n",
       "      <td>801.69</td>\n",
       "      <td>805.02</td>\n",
       "      <td>1,670,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1/23/2017</td>\n",
       "      <td>807.25</td>\n",
       "      <td>820.87</td>\n",
       "      <td>803.74</td>\n",
       "      <td>819.31</td>\n",
       "      <td>1,963,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1/24/2017</td>\n",
       "      <td>822.30</td>\n",
       "      <td>825.90</td>\n",
       "      <td>817.82</td>\n",
       "      <td>823.87</td>\n",
       "      <td>1,474,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1/25/2017</td>\n",
       "      <td>829.62</td>\n",
       "      <td>835.77</td>\n",
       "      <td>825.06</td>\n",
       "      <td>835.67</td>\n",
       "      <td>1,494,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1/26/2017</td>\n",
       "      <td>837.81</td>\n",
       "      <td>838.00</td>\n",
       "      <td>827.01</td>\n",
       "      <td>832.15</td>\n",
       "      <td>2,973,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1/27/2017</td>\n",
       "      <td>834.71</td>\n",
       "      <td>841.95</td>\n",
       "      <td>820.44</td>\n",
       "      <td>823.31</td>\n",
       "      <td>2,965,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1/30/2017</td>\n",
       "      <td>814.66</td>\n",
       "      <td>815.84</td>\n",
       "      <td>799.80</td>\n",
       "      <td>802.32</td>\n",
       "      <td>3,246,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1/31/2017</td>\n",
       "      <td>796.86</td>\n",
       "      <td>801.25</td>\n",
       "      <td>790.52</td>\n",
       "      <td>796.79</td>\n",
       "      <td>2,160,600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Open    High     Low   Close     Volume\n",
       "0    1/3/2017  778.81  789.63  775.80  786.14  1,657,300\n",
       "1    1/4/2017  788.36  791.34  783.16  786.90  1,073,000\n",
       "2    1/5/2017  786.08  794.48  785.02  794.02  1,335,200\n",
       "3    1/6/2017  795.26  807.90  792.20  806.15  1,640,200\n",
       "4    1/9/2017  806.40  809.97  802.83  806.65  1,272,400\n",
       "5   1/10/2017  807.86  809.13  803.51  804.79  1,176,800\n",
       "6   1/11/2017  805.00  808.15  801.37  807.91  1,065,900\n",
       "7   1/12/2017  807.14  807.39  799.17  806.36  1,353,100\n",
       "8   1/13/2017  807.48  811.22  806.69  807.88  1,099,200\n",
       "9   1/17/2017  807.08  807.14  800.37  804.61  1,362,100\n",
       "10  1/18/2017  805.81  806.21  800.99  806.07  1,294,400\n",
       "11  1/19/2017  805.12  809.48  801.80  802.17    919,300\n",
       "12  1/20/2017  806.91  806.91  801.69  805.02  1,670,000\n",
       "13  1/23/2017  807.25  820.87  803.74  819.31  1,963,600\n",
       "14  1/24/2017  822.30  825.90  817.82  823.87  1,474,000\n",
       "15  1/25/2017  829.62  835.77  825.06  835.67  1,494,500\n",
       "16  1/26/2017  837.81  838.00  827.01  832.15  2,973,900\n",
       "17  1/27/2017  834.71  841.95  820.44  823.31  2,965,800\n",
       "18  1/30/2017  814.66  815.84  799.80  802.32  3,246,600\n",
       "19  1/31/2017  796.86  801.25  790.52  796.79  2,160,600"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     325.25\n",
       "1     331.27\n",
       "2     329.83\n",
       "3     328.34\n",
       "4     322.04\n",
       "       ...  \n",
       "15    829.62\n",
       "16    837.81\n",
       "17    834.71\n",
       "18    814.66\n",
       "19    796.86\n",
       "Name: Open, Length: 1278, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3.3 Visualising the results\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.26571557023658"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = math.sqrt(mean_squared_error(real_stock_price, predicted_stock_price))\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "363px",
    "left": "74px",
    "right": "20px",
    "top": "133px",
    "width": "498px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
